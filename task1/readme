## Bigram Transformer Language Model

Простой пример Bigram Language Model на PyTorch с реализацией Transformer-блока.  
Вдохновлено мини-GPT проектом Андрея Карпати.

---

## Описание

Этот скрипт:
- Загружает датасет (`tiny Shakespeare`).
- Кодирует текст в токены (символы).
- Строит простой Bigram Transformer:
  - Включает позиционные эмбеддинги.
  - Несколько блоков `Multi-Head Self-Attention`.
  - Полносвязный слой (FeedForward).
- Обучает модель предсказывать следующий токен.
- Генерирует текст, начиная с начального токена.

---

## Гиперпараметры

- **batch_size** = 16  
- **block_size** = 32 (контекст)
- **n_embd** = 64 (размер эмбеддинга)
- **n_head** = 4 (количество голов внимания)
- **n_layer** = 4 (число Transformer-блоков)
- **dropout** = 0.0  
- **learning_rate** = 1e-3  
- **max_iters** = 5000 (итерации обучения)
