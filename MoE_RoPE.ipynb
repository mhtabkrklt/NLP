{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMEvh2vAWIbuZSvDJ8f7C8T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mhtabkrklt/NLP/blob/main/MoE_RoPE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## с добавление МoE"
      ],
      "metadata": {
        "id": "gc5CA8FHqL9Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 32 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.5\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "with open('input1.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.head_size = head_size  # добавляем сохранение размера головы\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    @staticmethod\n",
        "    def apply_rope(x, head_size):\n",
        "        # x: (B, T, head_size)\n",
        "        B, T, HS = x.shape\n",
        "        assert HS % 2 == 0, \"Head size must be even for RoPE\"\n",
        "\n",
        "        # Разделяем на четные и нечетные индексы\n",
        "        x1 = x[..., 0::2]\n",
        "        x2 = x[..., 1::2]\n",
        "\n",
        "        # Вычисляем частоты\n",
        "        theta = 10000 ** (-torch.arange(0, HS//2, device=x.device) / (HS//2))\n",
        "        position = torch.arange(T, device=x.device).unsqueeze(1)\n",
        "        freqs = position * theta.unsqueeze(0)  # (T, HS//2)\n",
        "\n",
        "        # Применяем вращение\n",
        "        sin = freqs.sin().unsqueeze(0)  # (1,T,HS//2)\n",
        "        cos = freqs.cos().unsqueeze(0)  # (1,T,HS//2)\n",
        "\n",
        "        x_rotated = torch.cat([\n",
        "            x1 * cos - x2 * sin,\n",
        "            x1 * sin + x2 * cos\n",
        "        ], dim=-1)\n",
        "\n",
        "        return x_rotated\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "\n",
        "        # Применяем линейные преобразования\n",
        "        q = self.query(x)  # (B,T,head_size)\n",
        "        k = self.key(x)    # (B,T,head_size)\n",
        "        v = self.value(x)  # (B,T,head_size)\n",
        "\n",
        "        # Применяем RoPE к q и k\n",
        "        q = self.apply_rope(q, self.head_size)\n",
        "        k = self.apply_rope(k, self.head_size)\n",
        "\n",
        "        # Вычисляем attention scores\n",
        "        wei = q @ k.transpose(-2, -1) * (self.head_size ** -0.5)  # (B,T,T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "\n",
        "        # Применяем attention к values\n",
        "        out = wei @ v  # (B,T,head_size)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Применяем каждую голову параллельно и конкатенируем результаты\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        # Применяем проекцию и dropout\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class MoEFeedForward(nn.Module):\n",
        "    \"\"\"Mixture of Experts (MoE) FeedForward layer\"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, num_experts=4, top_k=2):\n",
        "        super().__init__()\n",
        "        self.num_experts = num_experts\n",
        "        self.top_k = top_k\n",
        "        self.experts = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Linear(n_embd, 4 * n_embd),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(4 * n_embd, n_embd)\n",
        "            ) for _ in range(num_experts)\n",
        "        ])\n",
        "        self.gate = nn.Linear(n_embd, num_experts)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, T, C)\n",
        "        B, T, C = x.shape\n",
        "        x_flat = x.view(-1, C)  # (B*T, C)\n",
        "\n",
        "        # Gating (router)\n",
        "        gate_scores = self.gate(x_flat)  # (B*T, num_experts)\n",
        "        topk_scores, topk_indices = torch.topk(gate_scores, self.top_k, dim=-1)  # (B*T, top_k)\n",
        "\n",
        "        # Softmax over top-k\n",
        "        topk_weights = F.softmax(topk_scores, dim=-1)  # (B*T, top_k)\n",
        "\n",
        "        # Initialize output\n",
        "        out = torch.zeros_like(x_flat)  # (B*T, C)\n",
        "\n",
        "        for i in range(self.top_k):\n",
        "            expert_idx = topk_indices[:, i]  # (B*T,)\n",
        "            mask = torch.zeros(B*T, self.num_experts, device=x.device)\n",
        "            mask.scatter_(1, expert_idx.unsqueeze(1), 1.0)\n",
        "            mask = mask.bool()\n",
        "\n",
        "            for j, expert in enumerate(self.experts):\n",
        "                expert_mask = mask[:, j]\n",
        "                if expert_mask.any():\n",
        "                    expert_input = x_flat[expert_mask]\n",
        "                    expert_out = expert(expert_input)\n",
        "                    out[expert_mask] += topk_weights[expert_mask, i].unsqueeze(1) * expert_out\n",
        "\n",
        "        out = out.view(B, T, C)\n",
        "        return self.dropout(out)\n",
        "\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, dim: int, eps: float = 1e-6):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(dim))  # learnable scale\n",
        "\n",
        "    def _norm(self, x):\n",
        "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self._norm(x.float()).type_as(x)\n",
        "        return output * self.weight\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: attention + MoE FeedForward \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = MoEFeedForward(n_embd)  # заменено!\n",
        "        self.ln1 = RMSNorm(n_embd)\n",
        "        self.ln2 = RMSNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = RMSNorm(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "        # Инициализация весов\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        x = tok_emb\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzuxZ-DoHxWU",
        "outputId": "10ba078e-4494-4cae-83ca-f6a5d5a6a6d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.611135 M parameters\n",
            "step 0: train loss 4.7359, val loss 4.7343\n",
            "step 100: train loss 2.8002, val loss 2.8534\n",
            "step 200: train loss 2.6708, val loss 2.7358\n",
            "step 300: train loss 2.5818, val loss 2.6795\n",
            "step 400: train loss 2.5240, val loss 2.6201\n",
            "step 500: train loss 2.4796, val loss 2.5716\n",
            "step 600: train loss 2.4522, val loss 2.5682\n",
            "step 700: train loss 2.4178, val loss 2.5410\n",
            "step 800: train loss 2.3863, val loss 2.5219\n",
            "step 900: train loss 2.3700, val loss 2.5133\n",
            "step 1000: train loss 2.3468, val loss 2.4966\n",
            "step 1100: train loss 2.3256, val loss 2.4945\n",
            "step 1200: train loss 2.3121, val loss 2.4800\n",
            "step 1300: train loss 2.2975, val loss 2.4577\n",
            "step 1400: train loss 2.2860, val loss 2.4612\n",
            "step 1500: train loss 2.2710, val loss 2.4547\n",
            "step 1600: train loss 2.2521, val loss 2.4572\n",
            "step 1700: train loss 2.2448, val loss 2.4362\n",
            "step 1800: train loss 2.2209, val loss 2.4340\n",
            "step 1900: train loss 2.2177, val loss 2.4325\n",
            "step 2000: train loss 2.1957, val loss 2.4113\n",
            "step 2100: train loss 2.1821, val loss 2.4285\n",
            "step 2200: train loss 2.1809, val loss 2.4343\n",
            "step 2300: train loss 2.1656, val loss 2.4207\n",
            "step 2400: train loss 2.1462, val loss 2.4106\n",
            "step 2500: train loss 2.1469, val loss 2.4142\n",
            "step 2600: train loss 2.1373, val loss 2.4082\n",
            "step 2700: train loss 2.1306, val loss 2.4148\n",
            "step 2800: train loss 2.1100, val loss 2.4209\n",
            "step 2900: train loss 2.1157, val loss 2.4109\n",
            "step 3000: train loss 2.0986, val loss 2.3987\n",
            "step 3100: train loss 2.0993, val loss 2.4124\n",
            "step 3200: train loss 2.0751, val loss 2.4023\n",
            "step 3300: train loss 2.0669, val loss 2.3888\n",
            "step 3400: train loss 2.0642, val loss 2.3794\n",
            "step 3500: train loss 2.0602, val loss 2.3865\n",
            "step 3600: train loss 2.0504, val loss 2.3729\n",
            "step 3700: train loss 2.0326, val loss 2.3707\n",
            "step 3800: train loss 2.0296, val loss 2.3932\n",
            "step 3900: train loss 2.0182, val loss 2.3732\n",
            "step 4000: train loss 2.0135, val loss 2.3838\n",
            "step 4100: train loss 2.0053, val loss 2.3809\n",
            "step 4200: train loss 2.0051, val loss 2.3815\n",
            "step 4300: train loss 1.9877, val loss 2.3899\n",
            "step 4400: train loss 1.9800, val loss 2.3888\n",
            "step 4500: train loss 1.9721, val loss 2.3743\n",
            "step 4600: train loss 1.9740, val loss 2.3617\n",
            "step 4700: train loss 1.9674, val loss 2.3849\n",
            "step 4800: train loss 1.9523, val loss 2.3599\n",
            "step 4900: train loss 1.9555, val loss 2.3547\n",
            "step 4999: train loss 1.9385, val loss 2.3681\n",
            "\n",
            "Блюженосе,\n",
            "Котра спорножидушум.\n",
            "\n",
            "Еще го пологотые былоше,\n",
            "Нарпотьша по ним чанцей\n",
            "Он пле:, поильшем,\n",
            "Не моут клавый, что,\n",
            "Новочту поветажкам\n",
            "Поподаля генулорив в реслебе то слашыме и прой\n",
            "Он Дочей гаречустам;\n",
            "Я уменили надит.. зеждат.\n",
            "\n",
            "Кать уд блаталимон ней утама не вздалет.\n",
            "Офросик сомолет, улесте сместавный\n",
            "К с ним мог\n",
            "Ни мож разка Евгенный.\n",
            "От потек покуднонепрана;\n",
            "На, шудонных ним Ретобружши воей\n",
            "И сли тяз побый в для выли каокШамт;\n",
            "И имескукало слугую взе ин навам;\n",
            "И, злет де Отешкровутелдя его рерт;\n",
            "Аго копроия покум разгоходет,\n",
            "Ему прарткие\n",
            "Котщим он и пере;\n",
            "Кокую на взмительныи мнынь,\n",
            "Задляте лицы побез шевлютых нит;\n",
            "Сужкуют неожий был —\n",
            "Сво\n",
            "Длопалинных пемоем раздят;\n",
            "Я Евгенных кроненьебя насе!\n",
            "Приследе вень, потыннокогда\n",
            "Пери, субрачиенни гно:\n",
            "Клилыйх\n",
            "Котолива певсельным для вы,.\n",
            "Сво вонце нем нем\n",
            "И Адватсято мтом ботвлят\n",
            "Млерем\n",
            "Мить плостолполные шада!\n",
            "И баветаю Гремей угоски настой петела,\n",
            "За им стараха полодой любо.\n",
            "\n",
            "К лесных мгдубуршам;\n",
            "За гляги, пурски закительнный.\n",
            "Нескудая морузия хорной,\n",
            "Стеть унепливосен;\n",
            "Давал слослава воле:\n",
            "Ему цпосным f три ости:\n",
            "Подетлю как предривнепоимять,\n",
            "Мносятний вербра изстронил,\n",
            "Будет в постот ледим,\n",
            "Коит чед им сначто са\n",
            "Онег броютет носвои в манос,\n",
            "И имучесля жини! гругьна,\n",
            "И прежился стречивы носпо быза,\n",
            "Потали былалорной плесный.\n",
            "Леше свосей бим и сот,\n",
            "В детлин таматься, тожь\n",
            "Послошъезлосе баладое ип,\n",
            "Легки всесойны в сцет.\n",
            "Диться не вздесь не из\n",
            "Завнов ослал мой еего взержно в ред ида\n",
            "Ат этравный\n",
            "Смечтяться в не неде\n",
            "Отеге, (сломой го тонец,\n",
            "Дилым замляться стрем полумавоют,\n",
            "Услушавил в зонник погененым\n",
            "На стакальних был, нее седеника,\n",
            "Такчести бвесный, всеа долго;\n",
            "Евгенкино для снец; я вощий;\n",
            "Вс ножкин Еридмых горднь,\n",
            "С вет от дел — от вы, схов\n",
            "Снид готом ем пр веть,\n",
            "Смою переци, опетски то елов чтой!\n",
            "Он гострастат бзоробках;\n",
            "Ивмеравнил.сять, ныне дным,\n",
            "Шувю рузсо грунья тих гдам;\n",
            "Четы жин воле; сус уж плавый,\n",
            "Тощи бодой\n",
            "Людеть ем богажу сленьив,\n",
            "Его тери моснито\n",
            "Преченный ломные выли;\n",
            "Людоб мнвет голо вдено\n",
            "Его,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Результат**: loss еще ниже уменьшился\n"
      ],
      "metadata": {
        "id": "jyoLOLaGrc93"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# с добавление RoPE вместо обычного positional encodeing и доработал класс MultiHeadAttention . Loss стал ниже по сравнению с предыдущим вариантом"
      ],
      "metadata": {
        "id": "KFMO2-FaMoPx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# до: step 4999: train loss 2.0688, val loss 2.4153\n",
        "\n",
        "# после: step 4999: train loss 2.0459, val loss 2.3991"
      ],
      "metadata": {
        "id": "-W2i-ttuNSw0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 32 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.5\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input1.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.head_size = head_size  # добавляем сохранение размера головы\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    @staticmethod\n",
        "    def apply_rope(x, head_size):\n",
        "        # x: (B, T, head_size)\n",
        "        B, T, HS = x.shape\n",
        "        assert HS % 2 == 0, \"Head size must be even for RoPE\"\n",
        "\n",
        "        # Разделяем на четные и нечетные индексы\n",
        "        x1 = x[..., 0::2]\n",
        "        x2 = x[..., 1::2]\n",
        "\n",
        "        # Вычисляем частоты\n",
        "        theta = 10000 ** (-torch.arange(0, HS//2, device=x.device) / (HS//2))\n",
        "        position = torch.arange(T, device=x.device).unsqueeze(1)\n",
        "        freqs = position * theta.unsqueeze(0)  # (T, HS//2)\n",
        "\n",
        "        # Применяем вращение\n",
        "        sin = freqs.sin().unsqueeze(0)  # (1,T,HS//2)\n",
        "        cos = freqs.cos().unsqueeze(0)  # (1,T,HS//2)\n",
        "\n",
        "        x_rotated = torch.cat([\n",
        "            x1 * cos - x2 * sin,\n",
        "            x1 * sin + x2 * cos\n",
        "        ], dim=-1)\n",
        "\n",
        "        return x_rotated\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "\n",
        "        # Применяем линейные преобразования\n",
        "        q = self.query(x)  # (B,T,head_size)\n",
        "        k = self.key(x)    # (B,T,head_size)\n",
        "        v = self.value(x)  # (B,T,head_size)\n",
        "\n",
        "        # Применяем RoPE к q и k\n",
        "        q = self.apply_rope(q, self.head_size)\n",
        "        k = self.apply_rope(k, self.head_size)\n",
        "\n",
        "        # Вычисляем attention scores\n",
        "        wei = q @ k.transpose(-2, -1) * (self.head_size ** -0.5)  # (B,T,T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        wei = self.dropout(wei)\n",
        "\n",
        "        # Применяем attention к values\n",
        "        out = wei @ v  # (B,T,head_size)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Применяем каждую голову параллельно и конкатенируем результаты\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        # Применяем проекцию и dropout\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, dim: int, eps: float = 1e-6):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(dim))  # learnable scale\n",
        "\n",
        "    def _norm(self, x):\n",
        "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self._norm(x.float()).type_as(x)\n",
        "        return output * self.weight\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = RMSNorm(n_embd)\n",
        "        self.ln2 = RMSNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = RMSNorm(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "        # Инициализация весов\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        x = tok_emb\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZDMqygDtq9K",
        "outputId": "ebd58a79-1f77-47d5-d65e-69a4fd1815ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.213039 M parameters\n",
            "step 0: train loss 4.7317, val loss 4.7347\n",
            "step 100: train loss 2.8142, val loss 2.8631\n",
            "step 200: train loss 2.6593, val loss 2.7302\n",
            "step 300: train loss 2.5914, val loss 2.6808\n",
            "step 400: train loss 2.5337, val loss 2.6483\n",
            "step 500: train loss 2.4935, val loss 2.6162\n",
            "step 600: train loss 2.4735, val loss 2.5919\n",
            "step 700: train loss 2.4327, val loss 2.5492\n",
            "step 800: train loss 2.4223, val loss 2.5429\n",
            "step 900: train loss 2.3945, val loss 2.5342\n",
            "step 1000: train loss 2.3903, val loss 2.5221\n",
            "step 1100: train loss 2.3746, val loss 2.4982\n",
            "step 1200: train loss 2.3545, val loss 2.5006\n",
            "step 1300: train loss 2.3375, val loss 2.4853\n",
            "step 1400: train loss 2.3360, val loss 2.4879\n",
            "step 1500: train loss 2.3149, val loss 2.4855\n",
            "step 1600: train loss 2.3063, val loss 2.4841\n",
            "step 1700: train loss 2.2792, val loss 2.4703\n",
            "step 1800: train loss 2.2834, val loss 2.4674\n",
            "step 1900: train loss 2.2566, val loss 2.4524\n",
            "step 2000: train loss 2.2629, val loss 2.4626\n",
            "step 2100: train loss 2.2515, val loss 2.4574\n",
            "step 2200: train loss 2.2472, val loss 2.4508\n",
            "step 2300: train loss 2.2341, val loss 2.4327\n",
            "step 2400: train loss 2.2354, val loss 2.4442\n",
            "step 2500: train loss 2.2154, val loss 2.4321\n",
            "step 2600: train loss 2.2142, val loss 2.4403\n",
            "step 2700: train loss 2.2045, val loss 2.4371\n",
            "step 2800: train loss 2.1921, val loss 2.4298\n",
            "step 2900: train loss 2.1813, val loss 2.4255\n",
            "step 3000: train loss 2.1679, val loss 2.4276\n",
            "step 3100: train loss 2.1733, val loss 2.4252\n",
            "step 3200: train loss 2.1620, val loss 2.4293\n",
            "step 3300: train loss 2.1572, val loss 2.4120\n",
            "step 3400: train loss 2.1467, val loss 2.4212\n",
            "step 3500: train loss 2.1423, val loss 2.4354\n",
            "step 3600: train loss 2.1369, val loss 2.4110\n",
            "step 3700: train loss 2.1266, val loss 2.4025\n",
            "step 3800: train loss 2.1216, val loss 2.4109\n",
            "step 3900: train loss 2.1189, val loss 2.4113\n",
            "step 4000: train loss 2.0997, val loss 2.4037\n",
            "step 4100: train loss 2.1118, val loss 2.4238\n",
            "step 4200: train loss 2.1035, val loss 2.4149\n",
            "step 4300: train loss 2.0943, val loss 2.4084\n",
            "step 4400: train loss 2.0841, val loss 2.4074\n",
            "step 4500: train loss 2.0754, val loss 2.4058\n",
            "step 4600: train loss 2.0691, val loss 2.4017\n",
            "step 4700: train loss 2.0604, val loss 2.4039\n",
            "step 4800: train loss 2.0631, val loss 2.4109\n",
            "step 4900: train loss 2.0556, val loss 2.4138\n",
            "step 4999: train loss 2.0459, val loss 2.3991\n",
            "\n",
            "Болиц слек нисшким ядой\n",
            "Тотяей идатной се пород;\n",
            "Устрему иal'ste, дохствей, стровь;\n",
            "Даконих пастроспный,\n",
            "Ивриев полут;? Ювока\n",
            "Запия зму на оыло потят полад\n",
            "Там назвелья свы ших демупельйный,\n",
            "Ноги недал: рескоми Сь,\n",
            "Предара першант Моловнос наTnt и затилечьных валива браскень.;\n",
            "Я тенья зегоимтеных\n",
            "Бреном коонит прилью любы\n",
            "Ды радлив, в се пы, то ытолью вет,\n",
            "Бодни в каки пи он, тромв\n",
            "Готном от нар н вехердат..\n",
            "Я лелся но ж прялицлет..\n",
            "Рондосной людем моп.\n",
            "Бмивый недра стрыртья же,\n",
            "Ко ини шулый 1еды а теко.\n",
            "\n",
            "\n",
            "Рессловники сев, нает\n",
            "Презатаян встолки., получной,\n",
            "Не:\n",
            "И го покнучить плерстны.\n",
            "Ляи вел коских писенивной\n",
            "Бомуритьсновля длажать,\n",
            "Вздлал насчевой изночшных васаводный же поли,\n",
            "То доле: на нам празвтав? упривить и ститрат,\n",
            "Крачивож; драся можене,\n",
            "Ахдееничнивет Chatre1,\n",
            "Чет утосервавы, нашавал теших нежше ном\n",
            "Ау правску и полпонины —\n",
            "Чутять Емел предраму,\n",
            "Я томену; стер зла, супирмын стить плас;\n",
            "Малакок клящкуле, полья,\n",
            "Чуж мужний сваралыроневитет\n",
            "И мив жевно Вззлела мое,\n",
            "Оне 8 он мой змляскомой Шалчанале\n",
            "вельно твела ли в нам,\n",
            "Странаю стрегты во 9.:\n",
            "Подору.\n",
            "Лывироело, вит вой постоня!\n",
            "\n",
            "К сцовоствой почельножны,\n",
            "Натьки по-детьно раник,\n",
            "Воловиналисвышных нусикух\n",
            "Каданиювный, зытет,\n",
            "Заме фре взгрузи сшеле.\n",
            "\n",
            "Потрезской смрезнит упелленно сил ой долкбрид.\n",
            "И сыбоста ажаний их;\n",
            "Почной!\n",
            "Роствене паросета то порен..\n",
            "\n",
            "Свет, Трих омой с ревою мажеед;\n",
            "Хока всеный носпироженоей\n",
            "Узделсяшной ранам не мызне\n",
            "Ежго был всемилый оствою жень оде Свех изне вреда.\n",
            "Кул Фристальсце отов толкотно сконкоск.\n",
            "Пер ро, дравет деротей\n",
            "Потдалит; на забершчу,\n",
            "Грой удк их, гота,\n",
            "Свом тажно о обол убомел нетой!\n",
            "Пораломи бя жепни нМозоторенил.\n",
            "Полчтеный печепо Подледенуля скрочке,\n",
            "Емея, увы\n",
            "Оруду, часков, уздорьнодужд;\n",
            "Униилыя ж мас, слень,\n",
            "В онтых мы дран волв сечуши!\n",
            "К дих посена; вш лавсеменя:»\n",
            "В рагровы\n",
            "Любих могтлочны биитья\n",
            "И стамодной уж не пот,\n",
            "Тоять и пержу поку!\n",
            "Дведревам Средов\n",
            "Че стак ужет мражно у болуным,\n",
            "Дров Чтровилостну: причалый рог;\n",
            "К: озпыйный, ола ямускали:\n",
            "К поворнавал он\n",
            "Яль, ш\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "результат: Loss стал ниже по сравнению с вариантом без RoPE"
      ],
      "metadata": {
        "id": "CQbLWakdQVAx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6TsxGVbRYuMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ниже представлен код без RoPE"
      ],
      "metadata": {
        "id": "mpQjOoN9QI4_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 32 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.5\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "with open('input1.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, dim: int, eps: float = 1e-6):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(dim))  # learnable scale\n",
        "\n",
        "    def _norm(self, x):\n",
        "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self._norm(x.float()).type_as(x)\n",
        "        return output * self.weight\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = RMSNorm(n_embd)\n",
        "        self.ln2 = RMSNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = RMSNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DnkrL8wh0Yza",
        "outputId": "7601d2b5-64b5-47ca-e2de-cbb729891a7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.215087 M parameters\n",
            "step 0: train loss 4.8806, val loss 4.8903\n",
            "step 100: train loss 2.8519, val loss 2.9135\n",
            "step 200: train loss 2.7266, val loss 2.7981\n",
            "step 300: train loss 2.6760, val loss 2.7526\n",
            "step 400: train loss 2.6406, val loss 2.7238\n",
            "step 500: train loss 2.6088, val loss 2.7202\n",
            "step 600: train loss 2.5853, val loss 2.7007\n",
            "step 700: train loss 2.5656, val loss 2.6829\n",
            "step 800: train loss 2.5441, val loss 2.6613\n",
            "step 900: train loss 2.5386, val loss 2.6679\n",
            "step 1000: train loss 2.5198, val loss 2.6458\n",
            "step 1100: train loss 2.5148, val loss 2.6220\n",
            "step 1200: train loss 2.4839, val loss 2.6147\n",
            "step 1300: train loss 2.4735, val loss 2.6089\n",
            "step 1400: train loss 2.4495, val loss 2.5937\n",
            "step 1500: train loss 2.4299, val loss 2.5892\n",
            "step 1600: train loss 2.4143, val loss 2.5622\n",
            "step 1700: train loss 2.4002, val loss 2.5550\n",
            "step 1800: train loss 2.3798, val loss 2.5360\n",
            "step 1900: train loss 2.3709, val loss 2.5312\n",
            "step 2000: train loss 2.3571, val loss 2.5306\n",
            "step 2100: train loss 2.3475, val loss 2.5151\n",
            "step 2200: train loss 2.3286, val loss 2.5061\n",
            "step 2300: train loss 2.3140, val loss 2.4861\n",
            "step 2400: train loss 2.2988, val loss 2.4934\n",
            "step 2500: train loss 2.2844, val loss 2.4892\n",
            "step 2600: train loss 2.2804, val loss 2.4849\n",
            "step 2700: train loss 2.2648, val loss 2.4777\n",
            "step 2800: train loss 2.2593, val loss 2.4674\n",
            "step 2900: train loss 2.2433, val loss 2.4676\n",
            "step 3000: train loss 2.2393, val loss 2.4725\n",
            "step 3100: train loss 2.2287, val loss 2.4465\n",
            "step 3200: train loss 2.2162, val loss 2.4474\n",
            "step 3300: train loss 2.2053, val loss 2.4470\n",
            "step 3400: train loss 2.1974, val loss 2.4364\n",
            "step 3500: train loss 2.1959, val loss 2.4520\n",
            "step 3600: train loss 2.1858, val loss 2.4298\n",
            "step 3700: train loss 2.1714, val loss 2.4353\n",
            "step 3800: train loss 2.1681, val loss 2.4406\n",
            "step 3900: train loss 2.1535, val loss 2.4288\n",
            "step 4000: train loss 2.1510, val loss 2.4383\n",
            "step 4100: train loss 2.1380, val loss 2.4290\n",
            "step 4200: train loss 2.1318, val loss 2.4260\n",
            "step 4300: train loss 2.1322, val loss 2.4258\n",
            "step 4400: train loss 2.1238, val loss 2.4257\n",
            "step 4500: train loss 2.1124, val loss 2.4121\n",
            "step 4600: train loss 2.1091, val loss 2.4092\n",
            "step 4700: train loss 2.1058, val loss 2.4121\n",
            "step 4800: train loss 2.0753, val loss 2.4034\n",
            "step 4900: train loss 2.0806, val loss 2.4282\n",
            "step 4999: train loss 2.0688, val loss 2.4153\n",
            "\n",
            "Кра носка Помотыл по заняб утром,\n",
            "Сворностмой вчаленно n.\n",
            "\n",
            "Похот;\n",
            "В брелим прихне\n",
            "Всебнины о пона тридеть мидыля,\n",
            "Я мят; восланый не умизмну\n",
            "Ниве, крем го жило крудав\n",
            "И нем сокой сурубавений посрим,\n",
            "Свследон Фантопетялел убс\n",
            "Зала, настал дому не счаных.\n",
            "\n",
            "В це изпостат ни дуденый в Инед:\n",
            "Оногда епостончный увкяскаже!\n",
            "Всем, де в плердынный,\n",
            "И тед фих егина споладеняд,\n",
            "Просмома нешихотныени;\n",
            "Очасловат болесно же ког\n",
            "Прежелиюпольшель, обаножу чной.\n",
            "\n",
            "Коденный ихолою послеть,\n",
            "Оне нагон Исенных жечно смай\n",
            "Нета прошно я кащецу, нак низупольной\n",
            "Ч; кданта;\n",
            "И счу бул онляа хотибье:\n",
            "Кин сел там, повень,,\n",
            "Попичускину провний, невы,\n",
            "На, гили гория ук врассличат,\n",
            "И Он пы дочел, и дон развеньвою.\n",
            "Перниченны? эпвигеним имнокра хосподет но год,\n",
            "И рокда, са с ебецелае\n",
            "Отник сминел невл\n",
            "И то скиетрым\n",
            "И бя! вак раничноски; модный\n",
            "Ислий на сыл ою и игкФон и для:\n",
            "Б сташ умунбя жбердинных изни разлем,\n",
            "Драд вилой страня в адругин:\n",
            "Зашин  жизья быт, тынилышланном\n",
            "И оннука\n",
            "Он постермитов\n",
            "И свонорен: раномне мувной я бла,\n",
            "Нечныменья друсь седьмо в гони;\n",
            "\n",
            "И матсколов в блюд дою погно мое —\n",
            "Он рузра оные зрамы,\n",
            "Крук он коескому людой,\n",
            "Заз ренно ста. в ли быльно я в увежь\n",
            "Слате пориводя рако по уполный,\n",
            "Пом тларино 3.\n",
            "Прой толго тахсдитенник;\n",
            "Ничае тчереют в бедругаресть регначненной,\n",
            "К зотще по нежа к утри усполь;\n",
            "Свой то погибогостить посебдов\n",
            "И злильначальны, него ни страсал,\n",
            "Умо-фдань крав ходалия мне\n",
            "Брызвожа жизний стольноска\n",
            "Деная даслят пот полне нелиы,\n",
            "И зам носводил свосвороей,\n",
            "Ктененилсьергамой дазованска,\n",
            "Предалус,\n",
            "С Факол оне, не мазь устранюлюя,\n",
            "Кедой, вить и нов жиз и й«Евген;\n",
            "Сценивить скинное неши\n",
            "Полкучик летлита прерва;\n",
            "Перет\n",
            "Креднетем злентя генны\n",
            "Уещу..\n",
            "\n",
            "Бшлу зертили го оядим мой лоде,\n",
            "Зако свереднет обя, тахеи;\n",
            "Касдрете бкакенил, устерны;\n",
            "У к кон му. придах дего льюбен\n",
            "Серкую подна, коснак и порнебере.\n",
            "Дравинам праядла; баль придуданцум\n",
            "Полкамерк Меть мунет иame,\n",
            "Снедруго, славоставоттелде!\n",
            "И ни слкаденик серпиным\n",
            "К — мен сцаят.\n",
            "Он, злькоцни гражза!\n",
            "Кок нскупирочной сором,\n",
            "П\n"
          ]
        }
      ]
    }
  ]
}